---
layout: page
title: V2X-Real Multi-Agent Multi-Modal Model Application
description: Applying the multi-agent, multi-modal model trained on OPV2V simulation data to the V2X4Real real-world dataset.
img: 
importance: 4
category: technical
---

The **V2X-Real Multi-Agent Multi-Modal Model Application** project involves transferring and adapting a multi-agent, multi-modal model—originally trained on the **OPV2V** simulation dataset—to the **V2X4Real** real-world dataset. This transition is essential to bridge the gap between simulation and real-world scenarios, enhancing the model’s performance in cooperative perception using vehicle-to-vehicle (V2V) communication.

## Datasets:
- **OPV2V**: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. This dataset provides a robust environment for training models in a simulated world.
- **V2X4Real**: A Real-World Large-Scale Dataset for Vehicle-to-Vehicle Cooperation, designed to evaluate and implement cooperative perception models in actual driving environments.

## Key Objectives:
- **Model Transfer**: Applying the multi-agent, multi-modal model, which was fine-tuned using OPV2V simulation data, to the V2X4Real dataset.
- **Adaptation to Real-World Data**: Adjusting the model to work with real-world sensor data, including LiDAR and camera inputs from V2X4Real, ensuring accurate perception and vehicle-to-vehicle cooperation.
- **Data Integration & Validation**: Testing and validating the model's performance by comparing real-world results with simulated outcomes, making necessary corrections to the model’s training pipeline to ensure effective adaptation to real-world dynamics.

## Paper Submission
This project will be documented in a paper to be submitted to **CVPR in October 2024**, showcasing the application of the multi-agent, multi-modal model from simulation to a large-scale real-world cooperative perception dataset.
