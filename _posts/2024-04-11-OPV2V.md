---
layout: post
title: "Working with OPV2V and V2V4Real Datasets for Autonomous Driving"
date: 2024-04-11 10:00:00
description: "A guide to using OPV2V and V2V4Real datasets for vehicle-to-vehicle perception and cooperative perception in autonomous driving."
tags: opv2v v2v4real dataset autonomous-driving
categories: datasets
featured: true
---

In this post, we’ll explore the **OPV2V** and **V2V4Real** datasets, which are designed for vehicle-to-vehicle (V2V) cooperative perception in autonomous driving. We’ll also go over how to visualize the datasets and format your own data for machine learning applications.

### Dataset Overview

**Dataset** refers to a structured collection of data points that can be used for a specific purpose, such as training or testing machine learning models. Datasets can be stored in various formats, including:

- **Tabular** (CSV, Excel)
- **Images** (PNG, JPG)
- **Point Cloud Data** (PCD)

---

### OPV2V: Open Benchmark for Vehicle-to-Vehicle Perception

**OPV2V** is an open benchmark dataset and fusion pipeline designed to facilitate vehicle-to-vehicle (V2V) perception. The dataset is collected using **OPENCDA** in the **CARLA simulator**, making it an essential resource for researchers working on autonomous driving.

- [OPV2V Official Website](https://mobility-lab.seas.ucla.edu/opv2v/)
- [OPV2V GitHub Repository](https://github.com/DerrickXuNu/OpenCOOD)

  **Key Features**:

  - A dataset and pipeline for **V2V cooperative perception**.
  - Data collected using the **CARLA simulator** with **OPENCDA**.

  To get a feel for how the dataset looks, you can download a smaller scenario (less than 1GB) for testing:

  ````markdown
  ```bash
  # Download small dataset
  wget https://mobility-lab.seas.ucla.edu/opv2v/small_dataset.zip
  ```
  ````

  To run the OPV2V pipeline, you can follow the instructions in the OpenCOOD repository.

  ## V2V4Real: Real-World Vehicle-to-Vehicle Dataset

  V2V4Real is the first large-scale real-world dataset for vehicle-to-vehicle cooperative perception in autonomous driving. Unlike OPV2V, which is simulated, V2V4Real offers real-world data for training and evaluation.

  - **[V2V4Real Official Website](https://mobility-lab.seas.ucla.edu/v2v4real/)**: Visit the official V2V4Real website for more details on the dataset and its use in autonomous driving research.

  - **[V2V4Real GitHub Repository](https://github.com/ucla-mobility/v2v4real)**: Access the official GitHub repository for V2V4Real to explore the code, dataset, and installation instructions.

  - **[Research Paper](https://arxiv.org/abs/2303.07601)**: Read the research paper on V2V4Real, which provides in-depth insights into the dataset and its applications in cooperative vehicle-to-vehicle perception.

**Key Features**:
Real-world vehicle-to-vehicle dataset.
Large-scale data for cooperative perception.

    Here’s how you can access the dataset:
    ````markdown
    ```bash
    # Clone V2V4Real GitHub repository
    git clone https://github.com/ucla-mobility/v2v4real.git
    ```
    ````

**Dataset Use Cases**：
Use1: Visualizing a Single Frame
To visualize a single frame of point cloud data (PCD), you can use the pcl_viewer tool. Here’s how you can install and use it on Ubuntu:
``markdown
    `bash # Install pcl_viewer on Ubuntu
sudo apt install pcl-tools

    # Visualize a specific PCD file (e.g., 000000.pcd)
    pcl_viewer 000000.pcd
    ```
    ````
    This tool will allow you to inspect individual frames from the dataset, making it easier to understand the data structure and visualize 3D point clouds.

---

### How to Visualize Data from OPV2V

To visualize data from **OPV2V**, follow these steps:

1. **Install OpenCOOD**:

   - Clone the [OpenCOOD GitHub repository](https://github.com/DerrickXuNu/OpenCOOD).
   - Follow the installation instructions provided in the repository.

   ````
   ```bash
   # Clone the OpenCOOD repository
   git clone https://github.com/DerrickXuNu/OpenCOOD.git

   # Navigate to the directory
   cd OpenCOOD

   # Install required packages
   pip install -r requirements.txt
   ```
   ````

2. **Set Up a Python Virtual EnvironmentD**: Install PyTorch CPU version in a Python virtual environment:

   ````
   ```bash
   conda create --name open-cood python=3.7
   conda activate open-cood
   pip install torch==2.3.0+cpu torchvision==0.15.2+cpu -f https://download.pytorch.org/whl/torch_stable.html
   ```
   ````

3. **Extract the Dataset:**: Download and extract the OPV2V dataset Move the extracted dataset into the OpenCOOD directory:

   ````markdown
   ```bash
   # Move the extracted dataset to the test folder
   mv 2021_opv2v_data /path/to/OpenCOOD/opv2v_data_dumping/test/
   ```
   ````

4. **Modify the Visualization Configuration**: Update the visualization.yaml file in the OpenCOOD repository to specify the correct visualization mode.
   ````markdown
   ```bash
   visualize:
   mode: lidar  # Set the mode to visualize LIDAR data
   show: True   # Enable visualization
   ```
   ````
5. **Run the Visualization Tool**:
   Finally, run the visualization tool:
   ````markdown
   ```bash
   python opencood/tools/visualize.py
   ```
   ````

---

#### How to Format Your Own Data

When creating your own dataset for use in autonomous driving models, it’s essential to format it correctly for machine learning purposes. The data can be structured in several ways:

- Tabular format (e.g., CSV, Excel) for structured data like timestamps, sensor readings, or metadata.
- Images stored in directories, such as /images/train/ or /images/test/.
- Point cloud data (PCD) for LIDAR, stored in directories such as /pointclouds/.

Make sure the dataset format aligns with the model or framework you’re using. For example, OpenCOOD expects specific folder structures for different sensor inputs.#

### Conclusion

Both OPV2V and V2V4Real datasets provide valuable data for researching cooperative perception in autonomous driving. OPV2V gives you the flexibility of simulated data, while V2V4Real offers large-scale real-world data. Whether you’re visualizing point clouds or training models, these datasets can help push forward research in autonomous vehicle communication.

To get started, make sure to visit the following resources:
OPV2V Official Site
V2V4Real Official Site
OpenCOOD GitHub
